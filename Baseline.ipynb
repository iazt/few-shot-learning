{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class CIFAR10Train(Dataset):\n",
    "  def __init__(self, path):\n",
    "    dictionary = unpickle(path + 'data_batch_1')\n",
    "    self.images = dictionary['data']\n",
    "    self.images = self.images.reshape(-1,3,32,32)\n",
    "    self.images = (torch.Tensor(self.images)*2)/255 -1\n",
    "    \n",
    "    #images = images.movedim(1,3)\n",
    "    self.labels = dictionary['labels']\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.images.shape[0]\n",
    " \n",
    "  def __getitem__(self, index):\n",
    "    return self.labels[index], self.images[index]\n",
    "\n",
    "class CIFAR10Val(Dataset):\n",
    "  def __init__(self, path):\n",
    "    dictionary = unpickle(path + 'data_batch_2')\n",
    "    self.images = dictionary['data']\n",
    "    self.images = self.images.reshape(-1,3,32,32)\n",
    "    self.images = (torch.Tensor(self.images)*2)/255 -1\n",
    "    \n",
    "    #images = images.movedim(1,3)\n",
    "    self.labels = dictionary['labels']\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.images.shape[0]\n",
    " \n",
    "  def __getitem__(self, index):\n",
    "    return self.labels[index], self.images[index]\n",
    "\n",
    "class CIFAR10Test(Dataset):\n",
    "  def __init__(self, path):\n",
    "    dictionary = unpickle(path + 'test_batch')\n",
    "    self.images = dictionary['data']\n",
    "    self.images = self.images.reshape(-1,3,32,32)\n",
    "    self.images = (torch.Tensor(self.images)*2)/255 -1\n",
    "    \n",
    "    #images = images.movedim(1,3)\n",
    "    self.labels = dictionary['labels']\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.images.shape[0]\n",
    " \n",
    "  def __getitem__(self, index):\n",
    "    return self.labels[index], self.images[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "batch_size = 32\n",
    "dataset_train = CIFAR10Train('/content/cifar-10-batches-py/')\n",
    "dataset_val = CIFAR10Val('/content/cifar-10-batches-py/')\n",
    "dataset_test = CIFAR10Test('/content/cifar-10-batches-py/')\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineBB(nn.Module):\n",
    "\n",
    "  def __init__(self, nclasses):\n",
    "    super(MyNetSmall, self).__init__()\n",
    "\n",
    "    self.nclasses = nclasses\n",
    "    self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\n",
    "    self.conv2 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "    self.conv3 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "    self.conv4 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "    self.bn1 = torch.nn.BatchNorm2d(64)\n",
    "    self.bn2 = torch.nn.BatchNorm2d(64)\n",
    "    self.bn3 = torch.nn.BatchNorm2d(64)\n",
    "    self.bn4 = torch.nn.BatchNorm2d(64)\n",
    "    self.maxpool = torch.nn.MaxPool2d(2,2)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc1 = nn.Linear(2048,128)\n",
    "    self.fc2 = nn.Linear(128, self.nclasses)\n",
    "    self.softmax = nn.Softmax()\n",
    "    self.flatten = nn.Flatten()\n",
    "     \n",
    "  def forward(self, x):\n",
    "    x = self.bn1(self.relu(self.conv1(x)))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.bn2(self.relu(self.conv2(x)))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.bn3(self.relu(self.conv3(x)))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.bn4(self.relu(self.conv4(x)))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.relu(self.fc1(self.flatten(x)))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "def train(net, n_epochs, train_loader, val_loader):\n",
    "  total_train = len(train_loader)*train_loader.batch_size\n",
    "  total_val = len(val_loader)*val_loader.batch_size\n",
    "  val_acc = []\n",
    "  val_loss = []\n",
    "  t0 = time.time()\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "  for epoch in range(n_epochs):\n",
    "    net.train()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0): # Obtener batch\n",
    "      labels = data[0].cuda()\n",
    "      inputs = data[1].float().cuda()\n",
    "      optimizer.zero_grad()\n",
    "      outputs = net(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      # Calcular accuracy sobre el conjunto de validación y almacenarlo\n",
    "      # para hacer un plot después\n",
    "\n",
    "\n",
    "      items = (i+1) * train_loader.batch_size\n",
    "      running_loss += loss.item()\n",
    "      max_prob, max_idx = torch.max(outputs.data, dim=1)\n",
    "      running_acc += torch.sum(max_idx == labels).item()\n",
    "      info = f'\\rEpoch:{epoch+1}({items}/{total_train}), '\n",
    "      info += f'Loss:{running_loss/items:02.5f}, '\n",
    "      info += f'Train Acc:{running_acc/items*100:02.1f}%'\n",
    "      sys.stdout.write(info)\n",
    "\n",
    "    net.eval()\n",
    "    running_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "      labels = data[0].cuda()\n",
    "      inputs = data[1].float().cuda()\n",
    "      with torch.no_grad():\n",
    "        Y_pred = net(inputs)\n",
    "      max_prob, max_idx = torch.max(Y_pred.data, dim=1)\n",
    "      running_acc += torch.sum(max_idx == labels).item()\n",
    "      loss = criterion(Y_pred, labels)\n",
    "      # record validation loss\n",
    "      valid_loss+= loss.item()\n",
    "\n",
    "    val_acc.append(running_acc/total_val*100)  \n",
    "    val_loss.append(valid_loss/total_val*100)  \n",
    "    info = f', Val Acc:{running_acc/total_val*100:02.2f}%.\\n'\n",
    "    sys.stdout.write(info)\n",
    "  t1 = time.time()\n",
    "  return val_acc, val_loss, t1-t0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
